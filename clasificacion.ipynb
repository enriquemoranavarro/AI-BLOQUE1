{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmEo6WxYRCrZ",
        "outputId": "4098691f-f5b8-4fe6-a6a1-57745ef1031a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1' '0' '6.77864488255' ... '0.742605144742' '0.347446160889'\n",
            "  '0.131211251856']\n",
            " ['1' '1' '-0.354725173723' ... '2.01310088506' '1.28743035823'\n",
            "  '0.521263818116']\n",
            " ['1' '1' '1.72771550354' ... '0.288585321447' '0.67945383572'\n",
            "  '0.822154845116']\n",
            " ...\n",
            " ['2' '0' '2.0742480563' ... '1.18131645707' '1.38569938441'\n",
            "  '0.999760836326']\n",
            " ['2' '1' '-0.0564591439281' ... '0.742153560433' '0.124074097398'\n",
            "  '-0.0904859280961']\n",
            " ['2' '1' '-0.0483328162199' ... '1.04595614066' '0.737970144712'\n",
            "  '0.831975723135']]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "content = np.loadtxt('/content/P1_5.txt', dtype=str)\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1"
      ],
      "metadata": {
        "id": "eeZ17On6WCJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1"
      ],
      "metadata": {
        "id": "iIafEHtxYE0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer la primera columna que corresponde a las clases\n",
        "clases = content[:, 0]\n",
        "\n",
        "# Contar la frecuencia de cada clase y calcular su proporción\n",
        "clase_1_count = np.sum(clases == '1')\n",
        "clase_2_count = np.sum(clases == '2')\n",
        "total = len(clases)\n",
        "proporcion_clase_1 = clase_1_count / total\n",
        "proporcion_clase_2 = clase_2_count / total\n",
        "\n",
        "print(f\"Clase 1 (Atención): {clase_1_count} instancias ({proporcion_clase_1:.2%})\")\n",
        "print(f\"Clase 2 (Sin Atención): {clase_2_count} instancias ({proporcion_clase_2:.2%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cmyHxxEVELL",
        "outputId": "ff8d3317-c756-4918-a117-4415316c58ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 1 (Atención): 281 instancias (14.26%)\n",
            "Clase 2 (Sin Atención): 1689 instancias (85.74%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observando lo anterior podemos ver que los datos no están balanceados. Por lo tanto, aplicaré una estrategia de submuestreo de los datos mayoritarios para mitigar el hecho de que la muestra no esté balanceada."
      ],
      "metadata": {
        "id": "9bBQDjxtWDty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "clase_1_indices = np.where(content[:, 0] == '1')[0]\n",
        "clase_2_indices = np.where(content[:, 0] == '2')[0]\n",
        "\n",
        "# Submuestreo de la clase 2 (mayoritaria)\n",
        "np.random.seed(281)  # Número de instancias de la clase 1\n",
        "clase_2_submuestreada_indices = np.random.choice(\n",
        "    clase_2_indices, size=len(clase_1_indices), replace=False)\n",
        "\n",
        "# Combinar los índices submuestreados de la clase 2 con todos los índices de la clase 1\n",
        "indices_balanceados = np.concatenate([clase_1_indices, clase_2_submuestreada_indices])\n",
        "np.random.shuffle(indices_balanceados)\n",
        "datos_balanceados = content[indices_balanceados]\n",
        "\n",
        "print(\"Primeras filas del conjunto de datos balanceado:\")\n",
        "print(datos_balanceados[:10])\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piYyiCLVYDkd",
        "outputId": "a29b31a8-09dd-45b8-f4b1-c135dbf02cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras filas del conjunto de datos balanceado:\n",
            "[['2' '1' '-1.32031574568' ... '0.690991417864' '0.934780691239'\n",
            "  '1.22825612175']\n",
            " ['2' '1' '0.715251473588' ... '0.768991642588' '0.130260394012'\n",
            "  '0.211868715382']\n",
            " ['2' '1' '-1.59027205088' ... '2.34066981864' '1.84007890083'\n",
            "  '0.618981722881']\n",
            " ...\n",
            " ['2' '1' '-2.1078340269' ... '1.8729579623' '2.00573682622'\n",
            "  '1.39843651386']\n",
            " ['1' '1' '-0.367503011048' ... '0.21162335027' '0.199326074091'\n",
            "  '-0.538510096287']\n",
            " ['1' '1' '-1.5866503046' ... '1.14755695051' '1.04392262927'\n",
            "  '0.845145512013']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2"
      ],
      "metadata": {
        "id": "qMoADgDDeERR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "X = content[:, 2:].astype(float)\n",
        "y = content[:, 0].astype(int)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kmlPvWleN9B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "\n",
        "models = [\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=10000)),\n",
        "    ('Support Vector Classifier', SVC()),\n",
        "    ('Random Forest Classifier', RandomForestClassifier()),\n",
        "    ('Gradient Boosting Classifier', GradientBoostingClassifier()),\n",
        "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
        "    ('Naive Bayes', GaussianNB()),\n",
        "    ('Decision Tree Classifier', DecisionTreeClassifier()),\n",
        "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
        "    ('AdaBoost Classifier', AdaBoostClassifier())\n",
        "]\n",
        "\n",
        "results = {}\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models:\n",
        "    scores = []\n",
        "    for train_index, test_index in skf.split(X, y):\n",
        "        # Submuestreo dentro de cada pliegue\n",
        "        X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "        # Submuestreo de la clase mayoritaria\n",
        "        X_train_majority = X_train_fold[y_train_fold == 2]\n",
        "        y_train_majority = y_train_fold[y_train_fold == 2]\n",
        "        X_train_minority = X_train_fold[y_train_fold == 1]\n",
        "        y_train_minority = y_train_fold[y_train_fold == 1]\n",
        "\n",
        "        X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
        "            X_train_majority, y_train_majority,\n",
        "            replace=False,  # sin reemplazo\n",
        "            n_samples=len(y_train_minority),  # igualar el tamaño de la clase minoritaria\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Crear un nuevo conjunto de entrenamiento balanceado\n",
        "        X_train_balanced = np.vstack((X_train_minority, X_train_majority_downsampled))\n",
        "        y_train_balanced = np.concatenate((y_train_minority, y_train_majority_downsampled))\n",
        "\n",
        "        # Crear un pipeline para estandarizar los datos y aplicar el modelo\n",
        "        pipeline = make_pipeline(StandardScaler(), model)\n",
        "        pipeline.fit(X_train_balanced, y_train_balanced)\n",
        "        score = pipeline.score(X_test_fold, y_test_fold)\n",
        "        scores.append(score)\n",
        "\n",
        "    results[name] = np.mean(scores)\n",
        "\n",
        "for name, score in results.items():\n",
        "    print(f\"{name}: {score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW0y5en1eFOL",
        "outputId": "6abe356c-c953-4bb4-e16b-32308d3a9185"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: 0.84\n",
            "Support Vector Classifier: 0.85\n",
            "Random Forest Classifier: 0.84\n",
            "Gradient Boosting Classifier: 0.83\n",
            "K-Nearest Neighbors: 0.67\n",
            "Naive Bayes: 0.78\n",
            "Decision Tree Classifier: 0.71\n",
            "Quadratic Discriminant Analysis: 0.58\n",
            "AdaBoost Classifier: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo más efectivo es el support vector classifier."
      ],
      "metadata": {
        "id": "yP0irIvrgBnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3"
      ],
      "metadata": {
        "id": "vq3MbnSpjyys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_cost(y, y_pred):\n",
        "    m = len(y)\n",
        "    cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "    return cost\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, epochs):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)\n",
        "    costs = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        z = np.dot(X, theta)\n",
        "        y_pred = sigmoid(z)\n",
        "        gradient = (1/m) * np.dot(X.T, (y_pred - y))\n",
        "        theta -= learning_rate * gradient\n",
        "        cost = compute_cost(y, y_pred)\n",
        "        costs.append(cost)\n",
        "\n",
        "    return theta, costs\n",
        "\n",
        "def logistic_regression(X_train, y_train, X_test, y_test, learning_rate=0.01, epochs=1000):\n",
        "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
        "    X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
        "\n",
        "    theta, costs = gradient_descent(X_train, y_train, learning_rate, epochs)\n",
        "    y_pred_prob = sigmoid(np.dot(X_test, theta))\n",
        "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return accuracy, theta, costs\n",
        "\n",
        "X = content[:, 2:].astype(float)\n",
        "y = content[:, 0].astype(int)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Submuestreo de la clase mayoritaria\n",
        "    X_train_majority = X_train_fold[y_train_fold == 2]\n",
        "    y_train_majority = y_train_fold[y_train_fold == 2]\n",
        "    X_train_minority = X_train_fold[y_train_fold == 1]\n",
        "    y_train_minority = y_train_fold[y_train_fold == 1]\n",
        "\n",
        "    X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
        "        X_train_majority, y_train_majority,\n",
        "        replace=False,\n",
        "        n_samples=len(y_train_minority),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_train_balanced = np.vstack((X_train_minority, X_train_majority_downsampled))\n",
        "    y_train_balanced = np.concatenate((y_train_minority, y_train_majority_downsampled))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
        "    X_test_fold = scaler.transform(X_test_fold)\n",
        "\n",
        "    accuracy, theta, costs = logistic_regression(X_train_balanced, y_train_balanced, X_test_fold, y_test_fold)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "print(f'Precisión promedio del modelo de Regresión Logística: {np.mean(accuracies):.2f}')\n",
        "\n",
        "\n",
        "\n",
        "# Referencia https://www.geeksforgeeks.org/understanding-logistic-regression/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHq0N8rpjzYQ",
        "outputId": "de9ad4b8-98c9-4e52-d1a3-3367945c66ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-81159b6b8d0c>:14: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
            "<ipython-input-7-81159b6b8d0c>:14: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
            "<ipython-input-7-81159b6b8d0c>:14: RuntimeWarning: divide by zero encountered in log\n",
            "  cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión promedio del modelo de Regresión Logística: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4"
      ],
      "metadata": {
        "id": "p7fH66drm-bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "X = content[:, 2:].astype(float)\n",
        "y = content[:, 0].astype(int)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def feature_selection_and_evaluation(X, y, max_features):\n",
        "    best_accuracy = 0\n",
        "    best_num_features = 0\n",
        "    all_accuracies = []\n",
        "\n",
        "    for k in range(1, max_features + 1):\n",
        "        fold_accuracies = []\n",
        "\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "            y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "            X_train_majority = X_train_fold[y_train_fold == 2]\n",
        "            y_train_majority = y_train_fold[y_train_fold == 2]\n",
        "            X_train_minority = X_train_fold[y_train_fold == 1]\n",
        "            y_train_minority = y_train_fold[y_train_fold == 1]\n",
        "\n",
        "            X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
        "                X_train_majority, y_train_majority,\n",
        "                replace=False,\n",
        "                n_samples=len(y_train_minority),\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            X_train_balanced = np.vstack((X_train_minority, X_train_majority_downsampled))\n",
        "            y_train_balanced = np.concatenate((y_train_minority, y_train_majority_downsampled))\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
        "            X_test_fold = scaler.transform(X_test_fold)\n",
        "\n",
        "            selector = SelectKBest(f_classif, k=k)\n",
        "            X_train_selected = selector.fit_transform(X_train_balanced, y_train_balanced)\n",
        "            X_test_selected = selector.transform(X_test_fold)\n",
        "\n",
        "            model = SVC()\n",
        "            model.fit(X_train_selected, y_train_balanced)\n",
        "            y_pred = model.predict(X_test_selected)\n",
        "            accuracy = accuracy_score(y_test_fold, y_pred)\n",
        "            fold_accuracies.append(accuracy)\n",
        "\n",
        "        mean_accuracy = np.mean(fold_accuracies)\n",
        "        all_accuracies.append(mean_accuracy)\n",
        "\n",
        "        if mean_accuracy > best_accuracy:\n",
        "            best_accuracy = mean_accuracy\n",
        "            best_num_features = k\n",
        "\n",
        "    return best_num_features, best_accuracy, all_accuracies\n",
        "\n",
        "max_features = X.shape[1]\n",
        "best_num_features, best_accuracy, all_accuracies = feature_selection_and_evaluation(X, y, max_features)\n",
        "\n",
        "print(f'Número óptimo de características: {best_num_features}')\n",
        "print(f'Precisión del modelo con el número óptimo de características: {best_accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snI8AfRJm-2m",
        "outputId": "47e8f2c2-7dfd-41d9-a18e-40797b8624a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número óptimo de características: 141\n",
            "Precisión del modelo con el número óptimo de características: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5"
      ],
      "metadata": {
        "id": "s_j_RxIvogCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Usando una muestra más pequeña de los datos balanceados para acelerar el proceso (NO CORRÍA SI NO AGREGABA ESTE CÓDIGO)\n",
        "sample_indices = np.random.choice(X.shape[0], size=int(X.shape[0] * 0.1), replace=False)\n",
        "X_sample = X[sample_indices]\n",
        "y_sample = y[sample_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "svc = SVC(kernel='linear')  # Usar un kernel lineal para acelerar el cálculo\n",
        "sfs = SequentialFeatureSelector(svc, n_features_to_select='auto', direction='forward', scoring='accuracy', cv=5)\n",
        "\n",
        "# Crear un pipeline con estandarización y SFS\n",
        "pipeline = Pipeline([\n",
        "    ('selector', sfs),\n",
        "    ('classifier', svc)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "selected_features = sfs.get_support()\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Número de características seleccionadas: {np.sum(selected_features)}')\n",
        "print(f'Características seleccionadas: {np.where(selected_features)[0]}')\n",
        "print(f'Precisión del modelo con características seleccionadas: {accuracy:.2f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "7URHP6pOWUxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663799f8-8bd5-4864-e439-2d93fc9a27f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de características seleccionadas: 76\n",
            "Características seleccionadas: [  0   1   2   6   7   8   9  12  13  14  15  16  17  20  21  22  26  27\n",
            "  31  33  34  35  36  39  40  41  42  43  44  45  46  47  48  49  51  52\n",
            "  53  55  56  57  58  59  60  61  64  68  73  74  75  80  81  82  83  84\n",
            "  85  86  88  89  90 102 103 104 105 110 117 118 119 120 121 122 141 142\n",
            " 144 145 146 150]\n",
            "Precisión del modelo con características seleccionadas: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.6"
      ],
      "metadata": {
        "id": "s3RUljpdcv8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "accuracies = []\n",
        "selected_features_count = []\n",
        "feature_ranking = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Submuestreo de la clase mayoritaria\n",
        "    X_train_majority = X_train_fold[y_train_fold == 2]\n",
        "    y_train_majority = y_train_fold[y_train_fold == 2]\n",
        "    X_train_minority = X_train_fold[y_train_fold == 1]\n",
        "    y_train_minority = y_train_fold[y_train_fold == 1]\n",
        "\n",
        "    X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
        "        X_train_majority, y_train_majority,\n",
        "        replace=False,\n",
        "        n_samples=len(y_train_minority),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_train_balanced = np.vstack((X_train_minority, X_train_majority_downsampled))\n",
        "    y_train_balanced = np.concatenate((y_train_minority, y_train_majority_downsampled))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
        "    X_test_fold = scaler.transform(X_test_fold)\n",
        "\n",
        "    svc = SVC(kernel='linear')\n",
        "    rfecv = RFECV(estimator=svc, step=1, cv=5, scoring='accuracy')\n",
        "    rfecv.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    y_pred = rfecv.predict(X_test_fold)\n",
        "    accuracy = accuracy_score(y_test_fold, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    selected_features_count.append(rfecv.n_features_)\n",
        "    feature_ranking.append(rfecv.ranking_)\n",
        "\n",
        "print(f'Precisión promedio del modelo con características seleccionadas: {np.mean(accuracies):.2f}')\n",
        "print(f'Número promedio óptimo de características: {np.mean(selected_features_count):.2f}')\n",
        "print(f'Ranking de características promedio: {np.mean(feature_ranking, axis=0)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEjRsLaJcwqV",
        "outputId": "27b0b89c-c211-403e-c98b-28af8f792c56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisión promedio del modelo con características seleccionadas: 0.82\n",
            "Número promedio óptimo de características: 38.40\n",
            "Ranking de características promedio: [37.6 36.6 52.8 52.4 69.  64.6 27.8 22.2 10.2  1.   1.   2.6 73.2  4.6\n",
            "  2.4 32.8 34.2 30.8 55.6 36.  44.8 42.4 48.2  3.   9.8 31.2 38.6 19.\n",
            " 49.6 69.6 42.2  7.  57.  47.2 40.2 21.2 42.  67.  51.4  1.  38.8 63.8\n",
            " 14.8 47.6 45.4 45.2 52.2 45.2 38.  49.8 27.6 47.4 53.6 65.6 47.6 40.\n",
            " 48.6 50.  58.4 43.  57.8 61.4 46.8 50.2 97.6 26.   3.2 16.2 61.6 50.\n",
            " 40.  76.6 82.4 52.4 51.4 60.4 76.2 28.4 35.   7.6 53.8 42.  53.  74.2\n",
            " 81.8 67.4 45.2 76.4 64.2 43.4 20.2 29.4 59.8 60.8 71.8 59.8 87.  56.4\n",
            " 35.2 53.8 41.2 31.2 29.6 46.4 69.  39.  44.2 38.4 46.4 49.2 59.6 29.\n",
            "  8.4 61.8  8.6 38.2 80.4 81.4 52.  90.4 79.4 37.2  2.4 53.  43.  58.2\n",
            " 80.4 62.6 10.6 65.2 24.  47.6 41.6 77.4 41.4 42.8 77.6 69.2 42.4 60.\n",
            " 39.4 12.2 44.2 46.  67.4 68.2 44.8 41.4 47.  36.2 51.  55.8 34.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.7 (preguntar diferencia con la anterior)"
      ],
      "metadata": {
        "id": "Y7hT9_PXhKog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFECV\n",
        "\n",
        "accuracies = []\n",
        "selected_features_count = []\n",
        "feature_ranking = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
        "\n",
        "    # Submuestreo de la clase mayoritaria\n",
        "    X_train_majority = X_train_fold[y_train_fold == 2]\n",
        "    y_train_majority = y_train_fold[y_train_fold == 2]\n",
        "    X_train_minority = X_train_fold[y_train_fold == 1]\n",
        "    y_train_minority = y_train_fold[y_train_fold == 1]\n",
        "\n",
        "    X_train_majority_downsampled, y_train_majority_downsampled = resample(\n",
        "        X_train_majority, y_train_majority,\n",
        "        replace=False,\n",
        "        n_samples=len(y_train_minority),\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    X_train_balanced = np.vstack((X_train_minority, X_train_majority_downsampled))\n",
        "    y_train_balanced = np.concatenate((y_train_minority, y_train_majority_downsampled))\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
        "    X_test_fold = scaler.transform(X_test_fold)\n",
        "\n",
        "    svc = SVC(kernel='linear')\n",
        "    rfecv = RFECV(estimator=svc, step=1, cv=5, scoring='accuracy')\n",
        "    rfecv.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    y_pred = rfecv.predict(X_test_fold)\n",
        "    accuracy = accuracy_score(y_test_fold, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    selected_features_count.append(rfecv.n_features_)\n",
        "    feature_ranking.append(rfecv.ranking_)\n",
        "\n",
        "print(f'Precisión promedio del modelo con características seleccionadas: {np.mean(accuracies):.2f}')\n",
        "print(f'Número promedio óptimo de características: {np.mean(selected_features_count):.2f}')\n",
        "print(f'Ranking de características promedio: {np.mean(feature_ranking, axis=0)}')\n"
      ],
      "metadata": {
        "id": "AStwCtkbewH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33facd97-67c8-4289-a884-80c45c612f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número óptimo de características: 26\n",
            "Ranking de características: [ 21   1  15   1 113   1  10  13   1   1   1   1  48  11   1 126 125   7\n",
            "  95  94  52  24  12   1  90  38   1  56   9 111   4  51 102  39   2  49\n",
            "  50  61  25   1  26  31 101  40   1  64  65   1 100 124  17  89  60  59\n",
            "   1  19  75 104   1  58  66 106  41  69  76  57   1  22  70  87  37 114\n",
            "  81  86  85 110  72  97  84  23 107  63  98  67  96  68 105 119   1  62\n",
            "   1 103  99   1  46  47  34  45  14  92  93 118  18 122 121 117 116 115\n",
            "  30  88   1  29   1  73   1  82  83  74  16 120  36  35   1  91  20   1\n",
            " 108 109   5  54   1  44  43  77  42  27  32  79  80 123  28   3 128  78\n",
            " 112  55 127  53   1   8  33  71   6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.8"
      ],
      "metadata": {
        "id": "c3NL-s9LhMYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ¿Qué pasa si no se considera el problema de tener datos desbalanceados para este caso? ¿Por qué?\n",
        "Los modelos no hubieran sido tan precisos que que hubieramos contado con una clara falta de isntancias de una de las clases.\n",
        "2. De todos los clasificadores, ¿cuál o cuales consideras que son adecuados para los datos? ¿Qué propiedades tienen dichos modelos que los hacen apropiados para los datos? Argumenta tu respuesta.\n",
        "El SVC es el modelo más adecuado para las clases. El SVC se beneficia de maximizar el margen entre las clases, cosa que funciona aprticularmente bien con este set de datos. También, el SVC es más efectivo en espacios de alta dimensión. Cosa que es efectiva aquí\n",
        "3. ¿Es posibles reducir la dimensionalidad del problema sin perder rendimiento en el modelo? ¿Por qué?\n",
        "Si, si se hubiera trabajado con un sobreajuste como técnica de balanceo, sin embargo como trabajé con una técnica de subajuste no serviría reducir la dimensionalidad del problema sin afectar el rendimiento del modelo.\n",
        "4. ¿Qué método de selección de características consideras el más adecuado para este caso? ¿Por qué?\n",
        "Creo que el recursivo es mejor ya que por la simple cantidad de características que selecciona, lo vuelve más acertado.\n",
        "5. Si quisieras mejorar el rendimiento de tus modelos, ¿qué más se podría hacer?\n",
        "Podríamos hacer una busqueda y optimización de hiperparámetros para hacer el modelo más efectivo."
      ],
      "metadata": {
        "id": "p75ctHOXhNf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2"
      ],
      "metadata": {
        "id": "m0KqIA5yhjr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1"
      ],
      "metadata": {
        "id": "P3g0tBEChlV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "content = np.loadtxt('/content/M_3.txt', dtype=str)\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBzqjJEnhl56",
        "outputId": "a942cd7e-41f1-422c-c956-b7937adb55b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1' '1' '-1.55867656055' ... '0.292100379445' '-0.435294126191'\n",
            "  '1.3840817591']\n",
            " ['1' '1' '-1.97820748243' ... '0.134605372657' '-0.663639058713'\n",
            "  '1.23454478775']\n",
            " ['1' '1' '-2.00252078572' ... '0.207432345973' '-0.563342605951'\n",
            "  '1.04644537775']\n",
            " ...\n",
            " ['7' '1' '-4.9445669698' ... '-1.32610563077' '-0.601560252125'\n",
            "  '-5.29772692703']\n",
            " ['7' '1' '-3.65328964328' ... '-1.68888044294' '-0.297471104631'\n",
            "  '-5.37944000078']\n",
            " ['7' '1' '-4.66665759595' ... '-1.74725351068' '-0.371247669527'\n",
            "  '-5.16867605899']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraer la primera columna que corresponde a las clases\n",
        "clases = content[:, 0]\n",
        "\n",
        "# Contar la frecuencia de cada clase y calcular su proporción\n",
        "clase_1_count = np.sum(clases == '1')\n",
        "clase_2_count = np.sum(clases == '2')\n",
        "clase_3_count = np.sum(clases == '3')\n",
        "clase_4_count = np.sum(clases == '4')\n",
        "clase_5_count = np.sum(clases == '5')\n",
        "clase_6_count = np.sum(clases == '6')\n",
        "clase_7_count = np.sum(clases == '7')\n",
        "\n",
        "total = len(clases)\n",
        "\n",
        "proporcion_clase_1 = clase_1_count / total\n",
        "proporcion_clase_2 = clase_2_count / total\n",
        "proporcion_clase_3 = clase_3_count / total\n",
        "proporcion_clase_4 = clase_4_count / total\n",
        "proporcion_clase_5 = clase_5_count / total\n",
        "proporcion_clase_6 = clase_6_count / total\n",
        "proporcion_clase_7 = clase_7_count / total\n",
        "\n",
        "print(f\"Clase 1: {clase_1_count} instancias ({proporcion_clase_1:.2%})\")\n",
        "print(f\"Clase 2: {clase_2_count} instancias ({proporcion_clase_2:.2%})\")\n",
        "print(f\"Clase 3: {clase_3_count} instancias ({proporcion_clase_3:.2%})\")\n",
        "print(f\"Clase 4: {clase_4_count} instancias ({proporcion_clase_4:.2%})\")\n",
        "print(f\"Clase 5: {clase_5_count} instancias ({proporcion_clase_5:.2%})\")\n",
        "print(f\"Clase 6: {clase_6_count} instancias ({proporcion_clase_6:.2%})\")\n",
        "print(f\"Clase 7: {clase_6_count} instancias ({proporcion_clase_7:.2%})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erOAViGKiVkg",
        "outputId": "f9497d1d-916d-467e-a39d-fe3826cb15a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase 1: 90 instancias (14.29%)\n",
            "Clase 2: 90 instancias (14.29%)\n",
            "Clase 3: 90 instancias (14.29%)\n",
            "Clase 4: 90 instancias (14.29%)\n",
            "Clase 5: 90 instancias (14.29%)\n",
            "Clase 6: 90 instancias (14.29%)\n",
            "Clase 7: 90 instancias (14.29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "clase_1_indices = np.where(content[:, 0] == '1')[0]\n",
        "clase_2_indices = np.where(content[:, 0] == '2')[0]\n",
        "\n",
        "# Submuestreo de la clase 2 (mayoritaria)\n",
        "np.random.seed(281)  # Número de instancias de la clase 1\n",
        "clase_2_submuestreada_indices = np.random.choice(\n",
        "    clase_2_indices, size=len(clase_1_indices), replace=False)\n",
        "\n",
        "# Combinar los índices submuestreados de la clase 2 con todos los índices de la clase 1\n",
        "indices_balanceados = np.concatenate([clase_1_indices, clase_2_submuestreada_indices])\n",
        "np.random.shuffle(indices_balanceados)\n",
        "datos_balanceados = content[indices_balanceados]\n",
        "\n",
        "print(\"Primeras filas del conjunto de datos balanceado:\")\n",
        "print(datos_balanceados[:10])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCzQZBXCiWAB",
        "outputId": "ba3b2984-2b6b-40da-f775-efba3870a40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras filas del conjunto de datos balanceado:\n",
            "[['2' '1' '-2.6042329146' ... '-0.677647168021' '-0.943000508497'\n",
            "  '0.899909677596']\n",
            " ['2' '1' '-2.70038028644' ... '-0.639146593046' '-0.240777444659'\n",
            "  '-0.435886545211']\n",
            " ['2' '1' '-2.79801739582' ... '-1.22207511602' '-0.69849756082'\n",
            "  '0.0320692835053']\n",
            " ...\n",
            " ['1' '1' '-1.22710571275' ... '-0.188017608013' '-0.579827383565'\n",
            "  '1.0976972883']\n",
            " ['2' '1' '-1.84870482199' ... '-1.07974079532' '0.223656204654'\n",
            "  '0.854048882835']\n",
            " ['1' '1' '-0.84458637736' ... '1.40722591425' '0.299096588313'\n",
            "  '0.977341922083']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que los datos ya están balanceados, por lo que vamos a trabajar con el df original."
      ],
      "metadata": {
        "id": "4mCmKi14jEYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2"
      ],
      "metadata": {
        "id": "f5KZA7M5jLv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = content[:, 2:]  # Ignoramos la segunda columna y tomamos el resto como características\n",
        "y = content[:, 0].astype(int)  # La primera columna es la variable objetivo\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "models = [\n",
        "    ('Logistic Regression', LogisticRegression(max_iter=10000)),\n",
        "    ('Support Vector Classifier', SVC()),\n",
        "    ('Random Forest Classifier', RandomForestClassifier()),\n",
        "    ('Gradient Boosting Classifier', GradientBoostingClassifier()),\n",
        "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
        "    ('Naive Bayes', GaussianNB()),\n",
        "    ('Decision Tree Classifier', DecisionTreeClassifier()),\n",
        "    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis()),\n",
        "    ('AdaBoost Classifier', AdaBoostClassifier())\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for name, model in models:\n",
        "    # Crear un pipeline para estandarizar los datos y aplicar el modelo\n",
        "    pipeline = make_pipeline(StandardScaler(), model)\n",
        "    # Realizar validación cruzada y calcular la precisión promedio\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    results[name] = cv_scores.mean()\n",
        "\n",
        "for name, score in results.items():\n",
        "    print(f\"{name}: {score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4g6Bv8gjMjL",
        "outputId": "880d9535-12dc-4c7b-d383-573fc252c2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: 0.97\n",
            "Support Vector Classifier: 0.95\n",
            "Random Forest Classifier: 0.93\n",
            "Gradient Boosting Classifier: 0.90\n",
            "K-Nearest Neighbors: 0.93\n",
            "Naive Bayes: 0.86\n",
            "Decision Tree Classifier: 0.79\n",
            "Quadratic Discriminant Analysis: 0.27\n",
            "AdaBoost Classifier: 0.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3"
      ],
      "metadata": {
        "id": "1XNKh1yxXjPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X = content[:, 2:]\n",
        "y = content[:, 0].astype(int)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Parameter grid for Decision Tree Classifier\n",
        "param_grid_dt = {\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Parameter grid for SVC\n",
        "param_grid_svc = {\n",
        "    'C': np.logspace(-3, 2, 6),\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 5))\n",
        "}\n",
        "\n",
        "# Setup the models with GridSearchCV\n",
        "gs_dt = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid_dt, cv=cv_inner, scoring='accuracy')\n",
        "gs_svc = GridSearchCV(estimator=SVC(), param_grid=param_grid_svc, cv=cv_inner, scoring='accuracy')\n",
        "\n",
        "models = {\n",
        "    'SVC': gs_svc,\n",
        "    'Decision Tree': gs_dt\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_scaled, y, scoring='accuracy', cv=cv_outer)\n",
        "    print(f\"{name}: Mean Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores)*2:.2f})\")\n",
        "\n",
        "    model.fit(X_scaled, y)\n",
        "    print(f\"Best hyperparameters for {name}: {model.best_params_}\")\n"
      ],
      "metadata": {
        "id": "0lb3rhnOXj5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2301bd-4901-400c-f2bf-2fa806d1b9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC: Mean Accuracy: 0.97 (+/- 0.02)\n",
            "Best hyperparameters for SVC: {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Decision Tree: Mean Accuracy: 0.81 (+/- 0.03)\n",
            "Best hyperparameters for Decision Tree: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4"
      ],
      "metadata": {
        "id": "knZwlxE0hWnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preguntar diferencia con la anterior"
      ],
      "metadata": {
        "id": "vCSC1J8SiFKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Suponiendo que 'content' contiene los datos cargados\n",
        "X = content[:, 2:]  # Ignoramos la segunda columna y tomamos el resto como características\n",
        "y = content[:, 0].astype(int)  # La primera columna es la variable objetivo\n",
        "\n",
        "# Estandarizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Definir la estrategia de validación cruzada\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Definir los grids de hiperparámetros\n",
        "param_grid_dt = {\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "param_grid_svc = {\n",
        "    'C': np.logspace(-3, 2, 6),\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 1, 5))\n",
        "}\n",
        "\n",
        "# Configurar y ajustar GridSearchCV\n",
        "model_dt = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid_dt, cv=cv, scoring='accuracy')\n",
        "model_svc = GridSearchCV(SVC(), param_grid=param_grid_svc, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Ajustar los modelos\n",
        "model_dt.fit(X_scaled, y)\n",
        "model_svc.fit(X_scaled, y)\n",
        "\n",
        "\n",
        "print(\"Mejores hiperparámetros para Decision Tree:\", model_dt.best_params_)\n",
        "print(\"Mejores hiperparámetros para SVC:\", model_svc.best_params_)\n",
        "\n",
        "final_model_dt = DecisionTreeClassifier(**model_dt.best_params_)\n",
        "final_model_svc = SVC(**model_svc.best_params_)\n",
        "\n",
        "final_model_dt.fit(X_scaled, y)\n",
        "final_model_svc.fit(X_scaled, y)\n",
        "\n",
        "print(\"Modelos ajustados y listos para producción.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuIX8ouchf7t",
        "outputId": "47b2d7ab-5cb9-485c-cb8d-dc20093f3651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores hiperparámetros para Decision Tree: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
            "Mejores hiperparámetros para SVC: {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Modelos ajustados y listos para producción.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5"
      ],
      "metadata": {
        "id": "m7S_Ji3-j6hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contesta lo siguientes:\n",
        "1. ¿Observas un problema en cuanto al balanceo de las clases? ¿Por qué?\n",
        "En este set de datos que me tocó no fue necesario hacer un balanceo de los datos ya que las 7 clases contaban con la misma cantidad de instancias.\n",
        "2. ¿Qué modelo o modelos fueron efectivos para clasificar tus datos? ¿Observas algo especial sobre los modelos? Argumenta tu respuesta.\n",
        "La regresión logísitca y el SVC fueron los modelos más efectivos para clasificar mis datos. Algo que veo es que en ambos ejercicios el SVC fue de los más efectivos en todos momentos.\n",
        "3. ¿Observas alguna mejora importante al optimizar hiperparámetros? ¿Es el resultado que esperabas? Argumenta tu respuesta.\n",
        "Si, los modelos se vuelven ligeramente más precisos, exactamente un 0.02% mejores cada uno. Si esperaba eso ya que al encontrar los hiperparámetros podríamos trabajar con una slección muy conveniente de hiperparámetros.\n",
        "4. ¿Qué inconvenientes hay al encontrar hiperparámetros? ¿Por qué?\n",
        "Realmente el único inconveniente que experimenté al encontrar los hiperparámetros fue el costo computacional de este mismo. El código tardo más de un minuto en correr en ambos casos."
      ],
      "metadata": {
        "id": "LbJ9asjgj7G9"
      }
    }
  ]
}